#!/bin/sh

#!/bin/bash -e
#SBATCH -p tgac-medium # partition (queue)
#SBATCH -N 1 # number of nodes
#SBATCH -n 1 # number of tasks
#SBATCH --mem 8000 # memory pool for all cores
#SBATCH -t 0-15:00 # time (D-HH:MM)
#SBATCH -o slurm.%N.%j.out # STDOUT
#SBATCH -e slurm.%N.%j.err # STDERR
#SBATCH --mail-type=ALL # notifications for job done & fail
#SBATCH --mail-user=Tarang.Mehta@earlham.ac.uk # send-to address

################################################################################################################

# ATAC-seq pipeline - Part 3
# June 2020: Tarang K. Mehta, Earlham Institute, Norwich, UK

################################################################################################################

# Script usage: ./ATAC_Bioinf_pipeline_v2c.sh

## Place this script and the following files in $WD (created in first script)
# 1. As used in './ATAC_Bioinf_pipeline_v2a.sh': a 2-column space-delimited table where col1='R1/R2 filename's col2='desired species renamed filename: species_tissue_experiment e.g. Mz_L_ATAC/gDNA'
# 2. Scripts:
  # ATAC_Bioinf_pipeline_v2b_part5bD-a.py
  # ATAC_Bioinf_pipeline_v2b_part5bD.py
# 3. Run as an sbatch script with 8Gb memory and ~3 days runtime - will spawn off other jobs

################################################################################################################

# ~ This pipeline is ran for all ATAC narrow peak files generated by 'ATAC_Bioinf_pipeline_v2b.sh', and contains the following components:

# 1. IDR on all pairs of replicates (optional) - The IDR peaks are a subset of the naive overlap peaks that pass a specific IDR threshold of 10%.
# 	1a. IDR of true replicates
# 	1b. Compute Fraction of Reads in Peaks (FRiP) - bedtools and awk
# 2. TF footprinting and creation of signal tracks - RGT (HINT-ATAC)
# 3. Peak annotation - bedops and bioawk
# 	3a. TSS enrichment
# 	3b. Fraction of reads in annotated regions
# 4. Differential analysis of peaks - Homer and DiffBind

################################################################################################################

# All variables are added (and can be amended) here

scripts=(/tgac/workarea/group-vh/Tarang/ATACseq/2.run2) # place all scripts in the topmost directory - create this separately
# WD=(/tgac/workarea/group-vh/Tarang/ATACseq/2.run2/$spID) # insert the working directory
email=Tarang.Mehta@earlham.ac.uk # SBATCH out and err send to address

### 1. IDR
idrdir=($scripts/1.IDR) # assign raw reads dir
prefixATAC=($scripts/prefixATAC.txt)
prefixpairs=($idrdir/prefixpairs.txt)
idrpairs=($idrdir/idrpairpaths.txt)
idrpair1=($idrdir/idrpairpaths_rep1.txt)
idrpair2=($idrdir/idrpairpaths_rep2.txt)
idrprefix=($idrdir/idrprefix.txt)
idrpair1a=($idrdir/idrpairpaths_rep1unzipped.txt) # unzipped narrow peaks file path for rep1
idrpair2a=($idrdir/idrpairpaths_rep2unzipped.txt) # unzipped narrow peaks file path for rep2
IDR_THRESH=0.1 # consider changing to 0.05
IDR_THRESH_TRANSFORMED=$(awk -v p=${IDR_THRESH} 'BEGIN{print -log(p)/log(10)}')
npeaks_idr=($idrdir/idr_10thresh_peakcount.txt) # Number of peaks passing IDR thresholds of 10%
npeaks_idrrep1=($idrdir/idr_10thresh_peakcount_rep1.txt) # Number of peaks passing IDR thresholds of 10% for replicate 1
npeaks_idrrep2=($idrdir/idr_10thresh_peakcount_rep2.txt) # Number of peaks passing IDR thresholds of 10% for replicate 2

fripprefix=($idrdir/fripprefix.txt)
FRIP=($idrdir/FRiPvalues.txt)
PASS=0.3
ACCEPTABLE=0.2

### 2. TF footprinting and creation of signal tracks
tffprdir=$scripts/2.TFfprint_SignalTrack
splitmeme=$scripts/split_meme.py # ensure split meme python script is in scripts folder


## AS OF 25/06 - WHEN MOVED TO EI PROJECTS, REPLACE PATHS
# genome folders
Mzg=(/tgac/workarea/group-vh/Tarang/Reference_Genomes/ensembl/cichlids/Mzebra)
Png=(/tgac/workarea/group-vh/Tarang/Reference_Genomes/ensembl/cichlids/Pnyererei)
Abg=(/tgac/workarea/group-vh/Tarang/Reference_Genomes/ensembl/cichlids/Aburtoni)
Nbg=(/tgac/workarea/group-vh/Tarang/Reference_Genomes/ensembl/cichlids/Nbrichardi)
Ong=(/tgac/workarea/group-vh/Tarang/Reference_Genomes/ensembl/cichlids/Oniloticus)
# genome fasta - assign genomes here with FA* variables, and a list of paths will be created for while loops (so for adding own or other species, just change or add to variables here)
FAMzg=$Mzg/dna/Maylandia_zebra.M_zebra_UMD2a.dna.primary_assembly.allLG.fa
FAPng=$Png/dna/Pundamilia_nyererei.PunNye1.0.dna.nonchromosomal.fa
FAAbg=$Abg/dna/Haplochromis_burtoni.AstBur1.0.dna.nonchromosomal.fa
FANbg=$Nbg/dna/Neolamprologus_brichardi.NeoBri1.0.dna.nonchromosomal.fa
FAOng=$Ong/dna/Oreochromis_niloticus.O_niloticus_UMD_NMBU.dna.primary_assembly.allLG.fa
genfastas=(path_genomes.txt)
for gf in "${!FA@}"; do
  # echo "$gf is set to ${!gf}"
  echo "${!gf}" >> $genfastas
done

# chromosome sizes
Mzgchr=$Mzg/dna/Maylandia_zebra.M_zebra_UMD2a.dna.primary_assembly.allLG.fa.chrom.sizes
Pngchr=$Png/dna/Pundamilia_nyererei.PunNye1.0.dna.nonchromosomal.fa.chrom.sizes
Abgchr=$Abg/dna/Haplochromis_burtoni.AstBur1.0.dna.nonchromosomal.fa.chrom.sizes
Nbgchr=$Nbg/dna/Neolamprologus_brichardi.NeoBri1.0.dna.nonchromosomal.fa.chrom.sizes
Ongchr=$Ong/dna/Oreochromis_niloticus.O_niloticus_UMD_NMBU.dna.primary_assembly.allLG.fa.chrom.sizes
# annotation files •.gtf - assign annotations here with annot* variables, and a list of paths will be created for while loops (so for adding own or other species, just change or add to variables here)
annotMzg=$Mzg/current_gtf/maylandia_zebra/Maylandia_zebra.M_zebra_UMD2a.100.gtf.gz
annotPng=$Png/current_gtf/pundamilia_nyererei/Pundamilia_nyererei.PunNye1.0.100.gtf.gz
annotAbg=$Abg/current_gtf/haplochromis_burtoni/Haplochromis_burtoni.AstBur1.0.100.gtf.gz
annotNbg=$Nbg/current_gtf/neolamprologus_brichardi/Neolamprologus_brichardi.NeoBri1.0.100.gtf.gz
annotOng=$Ong/current_gtf/oreochromis_niloticus/Oreochromis_niloticus.O_niloticus_UMD_NMBU.100.gtf.gz
antfiles=(path_annot.txt)
for af in "${!annot@}"; do
  # echo "$af is set to ${!af}"
  echo "${!af}" >> $antfiles
done

# gene regions •.bed
Mzggen=$Mzg/current_gtf/maylandia_zebra/Maylandia_zebra.M_zebra_UMD2a.100.generegions.bed
Pnggen=$Png/current_gtf/pundamilia_nyererei/Pundamilia_nyererei.PunNye1.0.100.generegions.bed
Abggen=$Abg/current_gtf/haplochromis_burtoni/Haplochromis_burtoni.AstBur1.0.100.generegions.bed
Nbggen=$Nbg/current_gtf/neolamprologus_brichardi/Neolamprologus_brichardi.NeoBri1.0.100.generegions.bed
Onggen=$Ong/current_gtf/oreochromis_niloticus/Oreochromis_niloticus.O_niloticus_UMD_NMBU.100.generegions.bed
# assign the species you require for downloading BioMart gene alias annotations here - for my pipeline there are five species (but only four have biomart entries - for other species, check the biomart identifier for the database e.g. mzebra_gene_ensembl, and amend below)
# this will then use the variables below to add to a file 'biomart_sp.txt' - this will then be read, line by line to download and process the biomart databases
sp1=mzebra_gene_ensembl
sp2=pnyererei_gene_ensembl
sp3=hburtoni_gene_ensembl
sp4=oniloticus_gene_ensembl
#sp5=nbrichardi_gene_ensembl
biomartspecies=(biomart_sp.txt)
for var in "${!sp@}"; do
  # echo "$var is set to ${!var}"
  echo "${!var}" >> $biomartspecies
done
# tab delimited gene alias file - not required for pipeline but good for downstream analyses
Mzggenaltsv=$Mzg/current_gtf/maylandia_zebra/Maylandia_zebra.M_zebra_UMD2a.100.genealias.tsv
Pnggenaltsv=$Png/current_gtf/pundamilia_nyererei/Pundamilia_nyererei.PunNye1.0.100.genealias.tsv
Abggenaltsv=$Abg/current_gtf/haplochromis_burtoni/Haplochromis_burtoni.AstBur1.0.100.genealias.tsv
Nbggenaltsv=$Nbg/current_gtf/neolamprologus_brichardi/Neolamprologus_brichardi.NeoBri1.0.100.genealias.tsv
Onggenaltsv=$Ong/current_gtf/oreochromis_niloticus/Oreochromis_niloticus.O_niloticus_UMD_NMBU.100.genealias.tsv
# gene alias files *.txt
Mzggenal=$Mzg/current_gtf/maylandia_zebra/Maylandia_zebra.M_zebra_UMD2a.100.genealias.txt
Pnggenal=$Png/current_gtf/pundamilia_nyererei/Pundamilia_nyererei.PunNye1.0.100.genealias.txt
Abggenal=$Abg/current_gtf/haplochromis_burtoni/Haplochromis_burtoni.AstBur1.0.100.genealias.txt
Nbggenal=$Nbg/current_gtf/neolamprologus_brichardi/Neolamprologus_brichardi.NeoBri1.0.100.genealias.txt
Onggenal=$Ong/current_gtf/oreochromis_niloticus/Oreochromis_niloticus.O_niloticus_UMD_NMBU.100.genealias.txt
Usr='mehtat'
processggenaltsv=(processggenaltsvpath.txt)
biomartfiles=(biomartfilepaths.txt)
removesp=(neolamprologus_brichardi)
genalpaths=(genealiaspaths.txt)
rgtdatapath=(~/rgtdata) # insert path to RGT data folder for where RGT installed
rgtidsp1='[MzebraUMD2a]'
rgtidsp2='[PunNye1.0]'
rgtidsp3='[AstBur1.0]'
rgtidsp4='[NeoBri1.0]'
rgtidsp5='[OniloticusUMD]'

################################################################################################################

### 1. Irreproducible Discovery Rate (IDR) on MACS2 narrow peaks and FRiP

## It is worth following the ENCODE project’s “ATAC-seq Data Standards and Prototype Processing Pipeline” for replicated data on MACS2 peak calling:
  # Peak call with MACS2 > narrow peaks file > IDR on true replicates e.g. Ab5_L and Ab6_L: This is the dataset to use
    # IDR is A statistical procedure called the Irreproducible Discovery Rate (IDR) operates on the replicated peak set and compares consistency of ranks of these peaks in individual replicate/pseudoreplicate peak sets.
      # checks the reproducibility information from the duplicates using the IDR statistic
      # The basic idea is that if two replicates measure the same underlying biology, the most significant peaks, which are likely to be genuine signals, are expected to have high consistency between replicates, whereas peaks with low significance, which are more likely to be noise, are expected to have low consistency.
      # If the consistency between a pair of rank lists (peaks) that contains both significant and insignificant findings is plotted, a transition in consistency is expected
      # This consistency transition provides an internal indicator of the change from signal to noise and suggests how many peaks have been reliably detected - red being false, black being true
      # By fitting a bivariate rank distribuion, IDR finds a threshold to separate real peaks from noise
    # install idr locally: https://github.com/kundajelab/idr
      # wget https://github.com/kundajelab/idr/archive/2.0.4.zip
      # unzip 2.0.4.zip
      # cd idr-2.0.4/
      # ml gcc
      # ml zlib
      # python3 setup.py install
    # follow IDR details: https://docs.google.com/document/d/1f0Cm4vRyDQDu0bMehHD7P7KOMxTOP-HiNoIvL1VcBt8/edit#
      # set IDR_THRESH=0.1
    # If you have more than 2 true replicates select the longest peak list from all pairs that passes the IDR threshold.

## NOTE:
# 1. run IDR as an array for all replicate pairs
# 2. In cases where there are more than 2 true replicates select the longest peak list from all pairs that passes the IDR threshold.

mkdir -p $idrdir
cd $idrdir

# 1aA. create a 2-column tab delimited file that has all narrowpeak file paths of pairs to compare
# 1aB. split the above file into two files, one for each column
# 1aC. using an array by iterating through each line of the two 1-column files from above, do the following for each pair:
  # 1aCa. create a pooled-replicate narrowPeak file
  # 1aCb. run IDR on each pair and get peaks passing threshold
  # 1aCc. for cases where there are more than 2 true replicates, select the longest peak list from all pairs that passes the IDR threshold.

# =============================
# 1a. IDR of true replicates
# 1aA. Create IDR comparison pairs
# create a 2-column tab delimited file that has all narrowpeak file paths of pairs to compare
# =============================

for a in $(awk '{print $1}' $prefixATAC); do
  for b in $(awk '{print $1}' $prefixATAC); do
    # echo -e "$a\t$b"
    echo -e "$a\t$b" | awk '{if($1 != $2) print $1, $2;}' OFS='\t' | awk -F"_" '{if($2==$4) print $0}' |
    awk '{if (substr($1,1,2)==substr($2,1,2)) {print $0, "YES"} else if (substr($1,1,2)!=substr($2,1,2)) {print $0, "NO"}}' OFS='\t' |
    grep -v 'NO' | cut -f1,2 >> $prefixpairs.temp
  done
done

awk -F'\t' '!seen[$1>$2 ? $1 FS $2 : $2 FS $1]++' $prefixpairs.temp > $prefixpairs # this removes duplicate pairs that are simply in a different order
rm $prefixpairs.temp

awk -F'\t' '{print "/tgac/workarea/group-vh/Tarang/ATACseq/2.run2/"$1"/5.peak_calling/"$1"_peaks.narrowPeak.gz","\t","/tgac/workarea/group-vh/Tarang/ATACseq/2.run2/"$2"/5.peak_calling/"$2"_peaks.narrowPeak.gz"}' $prefixpairs > $idrpairs

# Run a file check to ensure the pairs exist and continue if they do, else, echo that they do not exist

while read -r r1 r2; do
  # echo "rep1 is $r1, rep2 is $r2"
  if test -f "$r1" && test -f "$r2"; then
    echo "$r1 and $r2 EXISTS"
    # =============================
    # 1aB. Separate the pairwise comparisons for an array
    # split the above file into two files, one for each column
    # =============================
    #
    cut -f1 $idrpairs > $idrpair1
    cut -f2 $idrpairs > $idrpair2
    paircount=$(wc -l $idrpairs | awk -F' ' '{print $1}') # assign variable for total number of pairs
    IDRarray=0-$(expr $paircount - 1) # number of pairs for the array starting from 0
    #
    # =============================
    # 1aC. Run the IDR analyses by iterating in an array
    # 1aCa. unzip files and create a pooled-replicate narrowPeak file
    # 1aCb. Perform IDR analysis.
      # Generate a plot and IDR output with additional columns including IDR scores.
    # 1aCc. Get peaks passing IDR threshold of 10%
    #
    while IFS= read -r i; do
      gunzip $i
    done < $idrpair1
    #
    while IFS= read -r i; do
      gunzip $i
    done < $idrpair2
    #
    sed 's/.gz//g' $idrpair1 > $idrpair1a
    sed 's/.gz//g' $idrpair2 > $idrpair2a
    #
    #
    echo '#!/bin/bash -e' > 1a.IDR.sh
    echo '#SBATCH -p tgac-short # partition (queue)' >> 1a.IDR.sh
    echo '#SBATCH -N 1 # number of nodes' >> 1a.IDR.sh
    echo '#SBATCH -n 1 # number of tasks' >> 1a.IDR.sh
    echo "#SBATCH --array=$IDRarray" >> 1a.IDR.sh
    echo '#SBATCH --mem-per-cpu 8000' >> 1a.IDR.sh
    echo '#SBATCH -t 0-00:45' >> 1a.IDR.sh
    echo '#SBATCH --mail-type=ALL # notifications for job done & fail' >> 1a.IDR.sh
    echo "#SBATCH --mail-user=$email # send-to address" >> 1a.IDR.sh
    echo '#SBATCH -o slurm.%N.%j.out # STDOUT' >> 1a.IDR.sh
    echo '#SBATCH -e slurm.%N.%j.err # STDERR' >> 1a.IDR.sh
    printf '\n' >> 1a.IDR.sh
    echo '# 1aC. Run the IDR analyses by iterating in an array' >> 1a.IDR.sh
    echo '# 1aCa. unzip files and create a pooled-replicate narrowPeak file' >> 1a.IDR.sh
    echo "mapfile -t idrrep1 < $idrpair1a" >> 1a.IDR.sh
    echo "mapfile -t idrrep2 < $idrpair2a" >> 1a.IDR.sh
    echo "awk '{print"' $1"_"$2}'"' $prefixpairs > $idrprefix" >> 1a.IDR.sh
    echo "mapfile -t idrprefix < $idrprefix" >> 1a.IDR.sh
    printf '\n' >> 1a.IDR.sh
    echo 'cat ${idrrep1[${SLURM_ARRAY_TASK_ID}]} ${idrrep2[${SLURM_ARRAY_TASK_ID}]} > ${idrprefix[${SLURM_ARRAY_TASK_ID}]}_peaks.narrowPeak'
    echo '# 1aCb. Perform IDR analysis.' >> 1a.IDR.sh
    echo '# Generate a plot and IDR output with additional columns including IDR scores.' >> 1a.IDR.sh
    echo 'srun idr --samples ${idrrep1[${SLURM_ARRAY_TASK_ID}]} ${idrrep2[${SLURM_ARRAY_TASK_ID}]} --peak-list ${idrprefix[${SLURM_ARRAY_TASK_ID}]}_peaks.narrowPeak --input-file-type narrowPeak --output-file ${idrprefix[${SLURM_ARRAY_TASK_ID}]}.IDR0.1output --rank p.value --soft-idr-threshold '"${IDR_THRESH} --plot --use-best-multisummit-IDR" >> 1a.IDR.sh
    echo '# 1aCc. Get peaks passing IDR threshold of 10%' >> 1a.IDR.sh
    echo '# IDR QC to report and using the IDR output' >> 1a.IDR.sh
    echo '# 1. For each biological replicate pair, filter the IDR peaks based on the ${IDR_THRESH_TRANSFORMED} and sort descending based on signal.value (col7)' >> 1a.IDR.sh
    printf '\n' >> 1a.IDR.sh
    echo "awk 'BEGIN{OFS="'"\t"} $12>='"'"'"${IDR_THRESH_TRANSFORMED}"'"' {print "'$1,$2,$3,$4,$5,$6,$7,$8,$9,$10}'"' "'${idrprefix[${SLURM_ARRAY_TASK_ID}]}.IDR0.1output | sort | uniq | sort -k7,7rn > ${idrprefix[${SLURM_ARRAY_TASK_ID}]}.IDR0.1Transf.narrowPeak' >> 1a.IDR.sh
    printf '\n' >> 1a.IDR.sh
    echo '# 2. For each biological replicate pair, count the number of lines passing ${IDR_THRESH_TRANSFORMED} - this is where IDR finds the threshold to separate real peaks from noise' >> 1a.IDR.sh
    echo 'wc -l ${idrprefix[${SLURM_ARRAY_TASK_ID}]}.IDR0.1Trans.narrowPeak >>'" $npeaks_idr # Number of peaks passing IDR thresholds of 10%" >> 1a.IDR.sh
    echo '# 3. For each biological replicate in a pair, assign the wc -l as max_numPeaks_Rep peaks' >> 1a.IDR.sh
    awk -F'.' '{print $1}' $npeaks_idr | awk -F'_' '{print $1"_"$2"_"$3" "$4"_"$5"_"$6}' | sed 's/ /\t/g' > $npeaks_idr.tmp
    echo "cut -f1,2 $npeaks_idr.tmp > $npeaks_idrrep1" >> 1a.IDR.sh
    echo "cut -f1,3 $npeaks_idr.tmp > $npeaks_idrrep2" >> 1a.IDR.sh
    echo '# cat $npeaks_idr.tmp2 $npeaks_idr.tmp3 | sort -k2,2 > $npeaks_idr2' >> 1a.IDR.sh
    echo "rm *tmp*" >> 1a.IDR.sh
    printf '\n' >> 1a.IDR.sh
    echo "# 4. For each biological replicate pair, sort the MACS2 narrowPeak file based on signal.value column (col7) and add another column on end 'max_numPeaks_Rep', adding 'T' for True and 'F' for False of peaks that pass IDR, based on wc -l" >> 1a.IDR.sh
    printf '\n' >> 1a.IDR.sh
    echo '# mapfile -t idrrep1 < $idrpair1a' >> 1a.IDR.sh
    echo '# mapfile -t idrrep2 < $idrpair2a' >> 1a.IDR.sh
    echo '# mapfile -t idrrep1peak < $npeaks_idrrep1' >> 1a.IDR.sh
    echo '# mapfile -t idrrep2peak < $npeaks_idrrep2' >> 1a.IDR.sh
    #
    #
    echo '#!/bin/bash -e' > 1aB.IDR.sh
    echo '#SBATCH -p tgac-medium # partition (queue)' >> 1aB.IDR.sh
    echo '#SBATCH -N 1 # number of nodes' >> 1aB.IDR.sh
    echo '#SBATCH -n 1 # number of tasks' >> 1aB.IDR.sh
    echo '#SBATCH --mem-per-cpu 8000' >> 1aB.IDR.sh
    echo '#SBATCH -t 0-02:45' >> 1aB.IDR.sh
    echo '#SBATCH --mail-type=ALL # notifications for job done & fail' >> 1aB.IDR.sh
    echo "#SBATCH --mail-user=$email # send-to address" >> 1aB.IDR.sh
    echo '#SBATCH -o slurm.%N.%j.out # STDOUT' >> 1aB.IDR.sh
    echo '#SBATCH -e slurm.%N.%j.err # STDERR' >> 1aB.IDR.sh
    printf '\n' >> 1aB.IDR.sh
    echo '# Double while read array:' >> 1aB.IDR.sh
    echo '# 1. Read in the peak numbers and narrowPeak files and do an IF statement to match the corresponding files' >> 1aB.IDR.sh
    echo '# 2. sort peaks file based on col7 (signal value - fold-change at peak summit)' >> 1aB.IDR.sh
    echo '# 3. read from file paths, open; read from peaks' >> 1aB.IDR.sh
    echo '# 4. use number in col1 to add T to that line and all preceding lines and add F for all lines thereafter' >> 1aB.IDR.sh
    printf '\n' >> 1aB.IDR.sh
    echo 'while IFS= read -u 3 -r reppeak && IFS= read -u 4 -r npeak; do' >> 1aB.IDR.sh
    echo -e '\t# echo ${npeak}' >> 1aB.IDR.sh
    echo -e '\t# echo ${reppeak}' >> 1aB.IDR.sh
    echo -e '\tpeakline=$(echo ${npeak} | awk -F'"' ' '{print "'$1}'"') # get the total count of IDR passed peaks" >> 1aB.IDR.sh
    echo -e '\tpeakprefix=$(echo ${npeak} | awk -F'"' ' '{print "'$2}'"') # get the file prefix this corresponds to" >> 1aB.IDR.sh
    echo -e '\t# echo $peakline' >> 1aB.IDR.sh
    echo -e '\t# echo $peakprefix' >> 1aB.IDR.sh
    echo -e '\tout=$(echo $(basename ${reppeak}) | sed -e '"'s/.narrowPeak/.final.narrowPeak/') # create an outfile" >> 1aB.IDR.sh
    echo -e '\tinprefix=$(echo $(basename ${reppeak}) | sed -e '"'s/_peaks.narrowPeak//') # get the prefix of the narrow peak in file" >> 1aB.IDR.sh
    echo -e '\t# echo $out' >> 1aB.IDR.sh
    echo -e '\t# echo $inprefix' >> 1aB.IDR.sh
    echo -e '\tif [[ $peakprefix = $inprefix ]]; then # check that the peaks passing IDR and narrowPeak file match' >> 1aB.IDR.sh
    echo -e '\t\techo "peakcount>> $peakprefix = $inprefix <<narrowPeaks_file: npeaks and peaks file ARE matched"'
    echo -e '\t\tsort -k7,7rn ${reppeak} | awk -v peakline="$peakline" '"'{if(NR>=1 && NR<=peakline)print "'$0,"T";else print $0,"F";}'"' OFS='\t' > "'${out} # check with awk '"'FNR>=103003 && FNR<=103006'" >> 1aB.IDR.sh
    echo -e '\telse' >> 1aB.IDR.sh
    echo -e '\t\techo "peakcount>> $peakprefix != $inprefix <<narrowPeaks_file: npeaks and peaks file NOT matched"' >> 1aB.IDR.sh
    echo -e '\tfi' >> 1aB.IDR.sh
    echo "done 3<$idrpair1a 4<$npeaks_idrrep1" >> 1aB.IDR.sh
  else
    echo "$r1 and/or $r2 DOES NOT EXIST"
  fi
done < idrpairpaths.txt


# IDR output
# Broad peak output files are the same except that they do not include the the summit columns (e.g. columns 10, 18, and 22 for samples with 2 replicates)
#
#     1. chrom string
#     Name of the chromosome for common peaks
#
#     2. chromStart int
#     The starting position of the feature in the chromosome or scaffold for common peaks, shifted based on offset. The first base in a chromosome is numbered 0.
#
#     3. chromEnd int
#     The ending position of the feature in the chromosome or scaffold for common peaks. The chromEnd base is not included in the display of the feature.
#
#     4. name string
#     Name given to a region (preferably unique) for common peaks. Use '.' if no name is assigned.
#
#     5. score int
#     Contains the scaled IDR value, min(int(log2(-125IDR), 1000). e.g. peaks with an IDR of 0 have a score of 1000, idr 0.05 have a score of int(-125log2(0.05)) = 540, and idr 1.0 has a score of 0.
#
#     6. strand [+-.] Use '.' if no strand is assigned.
#
#     7. signalValue float
#     Measurement of enrichment for the region for merged peaks. When a peak list is provided this is the value from the peak list.
#
#     8. p-value float
#     Merged peak p-value. When a peak list is provided this is the value from the peak list.
#
#     9. q-value float
#     Merged peak q-value. When a peak list is provided this is the value from the peak list.
#
#     10. summit int
#     Merged peak summit
#
#     11. localIDR float -log10(Local IDR value)
#
#     12. globalIDR float -log10(Global IDR value)
#
#     13. rep1_chromStart int
#     The starting position of the feature in the chromosome or scaffold for common replicate 1 peaks, shifted based on offset. The first base in a chromosome is numbered 0.
#
#     14. rep1_chromEnd int
#     The ending position of the feature in the chromosome or scaffold for common replicate 1 peaks. The chromEnd base is not included in the display of the feature.
#
#     15. rep1_signalValue float
#     Signal measure from replicate 1. Note that this is determined by the --rank option. e.g. if --rank is set to signal.value, this corresponds to the 7th column of the narrowPeak, whereas if it is set to p.value it corresponds to the 8th column.
#
#     16. rep1_summit int
#     The summit of this peak in replicate 1.
#
# [rep 2 data]
#
# ...
#
# [rep N data]


# The plot (*.IDR0.1output.png) for each quadrant is described below:
# Upper Left: Replicate 1 peak ranks versus Replicate 2 peak ranks - peaks that do not pass the specified idr threshold are colored red.
# Upper Right: Replicate 1 log10 peak scores versus Replicate 2 log10 peak scores - peaks that do not pass the specified idr threshold are colored red.
# Bottom Row: Peak rank versus IDR scores are plotted in black. The overlayed boxplots display the distribution of idr values in each 5% quantile. The IDR values are thresholded at the optimization precision - 1e-6 by default.

## FINAL PEAK FILES based on IDR QC are *.final.narrowPeak - the last column of T (True) or F (False) indicate peaks passing IDR

# =============================

echo '# -- 1. IDR started -- #'

JOBID1=$( sbatch -W --array=$IDRarray 1a.IDR.sh | awk '{print $4}' ) # Run the first job and then store the first job to variable JOBID1 (taken by awk once run)

JOBID2=$( sbatch -W --dependency=afterok:${JOBID1} 1aB.IDR.sh | awk '{print $4}' ) # JOB2 depends on JOB1 completing successfully

# =============================

# 1b. Compute Fraction of Reads in Peaks (FRiP)
# The fraction of reads in called peak regions (FRiP score) should be >0.3, though values greater than 0.2 are acceptable.
# FRiP scores will not be enforced as QC metric. TSS enrichment remains in place as a key signal to noise measure.

# 1bA. Create a 3 column file - col1: variableID; col2: tagAlign input; col3: IDR peak file

echo '#!/bin/bash -e' > 1bA.FRIPawk.sh
echo '#SBATCH -p tgac-short # partition (queue)' >> 1bA.FRIPawk.sh
echo '#SBATCH -N 1 # number of nodes' >> 1bA.FRIPawk.sh
echo '#SBATCH -n 1 # number of tasks' >> 1bA.FRIPawk.sh
echo '#SBATCH --mem-per-cpu 4000' >> 1bA.FRIPawk.sh
echo '#SBATCH -t 0-00:45' >> 1bA.FRIPawk.sh
echo '#SBATCH --mail-type=ALL # notifications for job done & fail' >> 1bA.FRIPawk.sh
echo "#SBATCH --mail-user=$email # send-to address" >> 1bA.FRIPawk.sh
echo '#SBATCH -o slurm.%N.%j.out # STDOUT' >> 1bA.FRIPawk.sh
echo '#SBATCH -e slurm.%N.%j.err # STDERR' >> 1bA.FRIPawk.sh
printf '\n' >> 1bA.FRIPawk.sh
echo 'awk -F'"'\t' '{print "'$1,"/tgac/workarea/group-vh/Tarang/ATACseq/2.run2/"$1"/5.peak_calling/"$1".tn5.tagAlign.gz",$1"_peaks.final.narrowPeak"}'"' OFS='\t' $prefixATAC > $fripprefix" >> 1bA.FRIPawk.sh


echo '# -- 1. IDR has completed -- #'

echo '# -- 2. FRiP calculation has started -- #'

JOBID3=$( sbatch -W --dependency=afterok:${JOBID2} 1bA.FRIPawk.sh | awk '{print $4}' ) # JOB3 depends on JOB2 completing successfully

echo '#!/bin/bash -e' > 1bB.FRIPok.sh
echo '#SBATCH -p tgac-short # partition (queue)' >> 1bB.FRIPok.sh
echo '#SBATCH -N 1 # number of nodes' >> 1bB.FRIPok.sh
echo '#SBATCH -n 1 # number of tasks' >> 1bB.FRIPok.sh
echo '#SBATCH --mem-per-cpu 1000' >> 1bB.FRIPok.sh
echo '#SBATCH -t 0-00:05' >> 1bB.FRIPok.sh
echo '#SBATCH --mail-type=ALL # notifications for job done & fail' >> 1bB.FRIPok.sh
echo "#SBATCH --mail-user=$email # send-to address" >> 1bB.FRIPok.sh
echo '#SBATCH -o slurm.%N.%j.out # STDOUT' >> 1bB.FRIPok.sh
echo '#SBATCH -e slurm.%N.%j.err # STDERR' >> 1bB.FRIPok.sh
printf '\n' >> 1bB.FRIPok.sh
echo 'FRiP calculation will start now...' >> 1bB.FRIPok.sh

JOBID4=$( sbatch -W --dependency=afterok:${JOBID3} 1bB.FRIPok.sh | awk '{print $4}' ) # JOB4 depends on JOB3 completing successfully

# 1bB. while read over the 3 column file and over each line
ml bedtools/2.25.0

while IFS=$'\t' read -r c1 c2 c3; do
  # assign variable for doing bedtools intersect of tagAlign and IDR
  val1=$(bedtools intersect -a <(zcat -f ${c2}) -b <(cat ${c3}) -wa -u | wc -l)
  # 1bC. For loop over the 3 column file and over each line, assigning variable.2 for wc -l over tagAlign file
  val2=$(zcat ${c2} | wc -l)
  # 1bD. For each pair of variables, calculate the FRiP value
  frac=$(bc -l <<< "(${val1}/${val2})")
  printf '%s\t%s\t%f\n' "$c1" "$val1""/""$val2" "$frac" >> $FRIP.temp
done < $fripprefix

while IFS=$'\t' read -r a b c; do
  if ((`bc <<< "${c}>=${PASS}"`));
  then printf '%s\t%s\t%f\t%s\n' "$a" "$b" "$c" "FRiP PASS" >> $FRIP
  elif ((`bc <<< "${c}>=${ACCEPTABLE}"`));
  then printf '%s\t%s\t%f\t%s\n' "$a" "$b" "$c" "FRiP PASS - acceptable" >> $FRIP
  else printf '%s\t%s\t%f\t%s\n' "$a" "$b" "$c" "FRiP FAIL" >> $FRIP
  fi
done < $FRIP.temp
rm $FRIP.temp

################################################################################################################

### 2. TF footprinting and creation of signal tracks

# Signal tracks are generated from BAM file (Raw) and bias corrected by HINT-ATAC
# This is rolled in with TF footprinting using HINT-ATAC, see this: https://www.regulatory-genomics.org/hint/tutorial/

# HINT-ATAC installed by installing RGT - Regulatory Genomics Toolbox: https://github.com/CostaLab/reg-gen
# install locally as config files need amending and other files need adding to the installation directory
# source ~/.bash_profile
# source ~/.bashrc
# ml gcc
# ml zlib
# conda install -c bioconda pybigwig
# pip install --user RGT


# CHECK THAT THE INPUT OF OUTPUT FROM ABOVE (*.final.narrowPeak) IS OK

# 1. Customise RGT data folder and data.config.user file for own genome files etc.: http://www.regulatory-genomics.org/rgt/rgt-data-folder/

mkdir -p $tffprdir
cd $tffprdir

# A. For each genome, the following is required:
# Aa. chromosome size files
source bioawk-1.0
while read -r a; do
  bioawk -c fastx '{ print $name, length($seq) }' < ${a} | awk '{print $1,$2}' OFS="\t" > $(echo "${a}" | sed 's/.fa/.fa.chrom.sizes/g')
done < $genfastas

# bioawk -c fastx '{ print $name, length($seq) }' < $AbgFA | awk '{print $1,$2}' OFS="\t" > $Abgchr
# bioawk -c fastx '{ print $name, length($seq) }' < $MzgFA | awk '{print $1,$2}' OFS="\t" > $Mzgchr
# bioawk -c fastx '{ print $name, length($seq) }' < $PngFA | awk '{print $1,$2}' OFS="\t" > $Pngchr
# bioawk -c fastx '{ print $name, length($seq) }' < $NbgFA | awk '{print $1,$2}' OFS="\t" > $Nbgchr
# bioawk -c fastx '{ print $name, length($seq) }' < $OngFA | awk '{print $1,$2}' OFS="\t" > $Ongchr

# Ab. gene regions file in bed format
while read -r b; do
  zcat ${b} | awk 'OFS="\t" {if ($3=="gene") {print $1,$4-1,$5,$10,$10,$7}}' | tr -d '";' > $(echo "${b}" | sed 's/.gtf.gz/.generegions.bed/g')
done < $antfiles

# zcat $Abgannot | awk 'OFS="\t" {if ($3=="gene") {print $1,$4-1,$5,$10,$10,$7}}' | tr -d '";' > $Abggen
# zcat $Mzgannot | awk 'OFS="\t" {if ($3=="gene") {print $1,$4-1,$5,$10,$10,$7}}' | tr -d '";' > $Mzggen
# zcat $Pngannot | awk 'OFS="\t" {if ($3=="gene") {print $1,$4-1,$5,$10,$10,$7}}' | tr -d '";' > $Pnggen
# zcat $Nbgannot | awk 'OFS="\t" {if ($3=="gene") {print $1,$4-1,$5,$10,$10,$7}}' | tr -d '";' > $Nbggen
# zcat $Ongannot | awk 'OFS="\t" {if ($3=="gene") {print $1,$4-1,$5,$10,$10,$7}}' | tr -d '";' > $Onggen

# Ac. gene alias file in text format - prepare from the gtf and BioMart

# Ac-1. awk from gtf to create a 2 colum file; col1=ensemblID, col2=gene_symbol (NA if 'ensembl' as symbol)- $file1
printf 'ensembl_gene_id\tensembl_gene_symbol\n' > tmpgenealias_header.txt
while read -r c; do
# for i in "$Mzgannot" "$Pngannot" "$Abgannot" "$Nbgannot" "$Ongannot"; do
  zcat ${c} | awk 'OFS="\t" {if ($3=="gene") {print $0}}' |
  awk '{print $10,$14}' OFS='\t' |
  sed 's/"//g' | sed 's/;//g' |
  awk '{$2=tolower($2);print}' OFS='\t' |
  sed 's/ensembl/NA/g' > $(echo "${c}" | sed 's/.gtf.gz/.genealias.txt.tmp0/g')
  cat tmpgenealias_header.txt $(echo "${c}" | sed 's/.gtf.gz/.genealias.txt.tmp0/g') > $(echo "${c}" | sed 's/.gtf.gz/.genealias.txt.tmp1/g')
  rm $(echo "${c}" | sed 's/.gtf.gz/.genealias.txt.tmp0/g')
done < $antfiles

# Ac-2. Download the following for each genome from BIOMART (below are the col headers)
  # ensembl_gene_id
  # ensembl_gene_id_version
  # ensembl_transcript_id
  # ensembl_transcript_id_version
  # hgnc_id
  # hgnc_symbol
  # entrezgene_accession
  # refseq_mrna_predicted
  # uniprotswissprot
  # wikigene_name
  # zfin_id_id
  # wikigene_id

# *- Go to here: https://m.ensembl.org/biomart/martview/246998056a05b7c965d983144dd0ddf6
# *-- Select 'Database' of genes and 'Attributes'
# *--- Once selected, click xml, copy and paste onto one line after wget -O result.txt 'http://www.ensembl.org/biomart/martservice?query=[INSERT XML HERE ON ONE LINE]'
# *---- Saved file will be stored as 'result.txt' so rename
# *----- Simply change the genome in '<Dataset name = "hburtoni_gene_ensembl"' to download other sets
  # mzebra_gene_ensembl
  # pnyererei_gene_ensembl
  # hburtoni_gene_ensembl
  # nbrichardi_gene_ensembl - not available
  # oniloticus_gene_ensembl

echo '#!/bin/bash -e' > 2.2_biomart_dl.sh
echo '#SBATCH -p ei-medium # partition (queue)' >> 2.2_biomart_dl.sh
echo '#SBATCH -N 1 # number of nodes' >> 2.2_biomart_dl.sh
echo '#SBATCH -c 1 # number of cores' >> 2.2_biomart_dl.sh
echo '#SBATCH --mem 8000 # memory pool for all cores' >> 2.2_biomart_dl.sh
echo '#SBATCH -t 0-2:59 # time (D-HH:MM)' >> 2.2_biomart_dl.sh
echo '#SBATCH -o slurm.%j.out # STDOUT' >> 2.2_biomart_dl.sh
echo '#SBATCH -e slurm.%j.err # STDERR' >> 2.2_biomart_dl.sh
echo '#SBATCH --mail-type=END,FAIL,TIME_LIMIT_75 # notifications for job done & fail' >> 2.2_biomart_dl.sh
echo '#SBATCH --mail-user=Tarang.Mehta@earlham.ac.uk # send-to addressUSERNAME=mehtat' >> 2.2_biomart_dl.sh
printf '\n' >> 2.2_biomart_dl.sh
echo "# this script will access the software node to download the biomart dbs" >> 2.2_biomart_dl.sh
echo 'source wget-1.14' >> 2.2_biomart_dl.sh
echo "USERNAME=$Usr" >> 2.2_biomart_dl.sh
echo 'HOSTNAME="software"' >> 2.2_biomart_dl.sh
echo "PWD=$(pwd)" >> 2.2_biomart_dl.sh
printf '\n' >> 2.2_biomart_dl.sh
echo 'SCRIPT="cd ${PWD}; sh 2.2_biomart_dl_script.sh"' >> 2.2_biomart_dl.sh
printf '\n' >> 2.2_biomart_dl.sh
echo "cd ${PWD}" > 2.2_biomart_dl_script.sh
echo "while read -r i; do" >> 2.2_biomart_dl_script.sh
echo -e '\twget -O ${i}_biomart1.txt '"'"'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "'"'"'${i}'"'"'" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "ensembl_gene_id_version" /><Attribute name = "ensembl_transcript_id" /><Attribute name = "ensembl_transcript_id_version" /><Attribute name = "hgnc_id" /></Dataset></Query>'"'" >> 2.2_biomart_dl_script.sh
echo -e '\twget -O ${i}_biomart2.txt '"'"'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "'"'"'${i}'"'"'" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "hgnc_symbol" /><Attribute name = "entrezgene_accession" /><Attribute name = "refseq_mrna_predicted" /></Dataset></Query>'"'" >> 2.2_biomart_dl_script.sh
echo -e '\twget -O ${i}_biomart3.txt '"'"'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "'"'"'${i}'"'"'" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "uniprotswissprot" /><Attribute name = "wikigene_name" /></Dataset></Query>'"'" >> 2.2_biomart_dl_script.sh
echo -e '\twget -O ${i}_biomart4.txt '"'"'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "'"'"'${i}'"'"'" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "zfin_id_id" /><Attribute name = "wikigene_id" /></Dataset></Query>'"'" >> 2.2_biomart_dl_script.sh
echo "done < $biomartspecies" >> 2.2_biomart_dl_script.sh
echo 'exit' >> 2.2_biomart_dl_script.sh
echo 'ssh -o StrictHostKeyChecking=no -l ${USERNAME} ${HOSTNAME} "${SCRIPT}"' >> 2.2_biomart_dl.sh
printf '\n' >> 2.2_biomart_dl.sh
echo "printf 'ensembl_transcript_id\thgnc_symbol\tentrezgene_accession\trefseq_mrna_predicted\tensembl_gene_id\tensembl_gene_id_version\tensembl_transcript_id\tensembl_transcript_id_version\thgnc_id\tensembl_transcript_id\tuniprotswissprot\twikigene_name\tensembl_transcript_id\tzfin_id_id\twikigene_id\n' > "'biomart_headers # NOTE - many of these cols will get removed later' >> 2.2_biomart_dl.sh
echo "while read -r i; do" >> 2.2_biomart_dl.sh
echo -e "\tawk '"'!$5{print $0,"NA";next}1'"' "'${i}_biomart1.txt > ${i}_biomart1a.txt # fill the 5th column with NA if empty' >> 2.2_biomart_dl.sh
echo -e "\tawk '"'!$2{print $0,"NA";next}1'"' "'${i}_biomart2.txt | awk '"'"'!$3{print $0,"NA";next}1'"' | awk '"'!$4{print $0,"NA";next}1'"' > "'${i}_biomart2a.txt # fill the 2nd, 3rd and 4th column with NA if empty' >> 2.2_biomart_dl.sh
echo -e "\tawk '"'!$2{print $0,"NA";next}1'"' "'${i}_biomart3.txt | awk '"'"'!$3{print $0,"NA";next}1'"' > "'${i}_biomart3a.txt # fill the 2nd and 3rd column with NA if empty' >> 2.2_biomart_dl.sh
echo -e "\tawk '"'!$2{print $0,"NA";next}1'"' "'${i}_biomart4.txt | awk '"'"'!$3{print $0,"NA";next}1'"' > "'${i}_biomart4a.txt # fill the 2nd and 3rd column with NA if empty' >> 2.2_biomart_dl.sh
echo -e '\tawk '"'BEGIN{OFS="'"\t"}NR==FNR{a[$3]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA","NA","NA";}}'"' "'${i}_biomart1a.txt ${i}_biomart2a.txt > ${i}_biomart1-2a.txt' >> 2.2_biomart_dl.sh
echo -e '\tawk '"'BEGIN{OFS="'"\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}'"' "'${i}_biomart3a.txt ${i}_biomart1-2a.txt > ${i}_biomart1-2-3a.txt' >> 2.2_biomart_dl.sh
echo -e '\tawk '"'BEGIN{OFS="'"\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}'"' "'${i}_biomart4a.txt ${i}_biomart1-2-3a.txt > ${i}_biomart.tmp.txt' >> 2.2_biomart_dl.sh
echo -e '\tcat biomart_headers ${i}_biomart.tmp.txt | awk '"'{print "'$1,$2,$3,$4,$5,$6,$8,$9,$11,$12,$14,$15}'"' OFS='\t' | awk '{print "'$5,$6,$1,$7,$8,$2,$3,$4,$9,$10,$11,$12}'"' OFS='\t' > "'${i}_biomart.txt' >> 2.2_biomart_dl.sh
echo -e '\trm ${i}_biomart.tmp.txt' >> 2.2_biomart_dl.sh
echo "done < $biomartspecies" >> 2.2_biomart_dl.sh


# # while loop original placed in script above, and a while loop version of the longer version below
# while read -r i; do
#   wget -O ${i}_biomart1.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "'${i}'" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "ensembl_gene_id_version" /><Attribute name = "ensembl_transcript_id" /><Attribute name = "ensembl_transcript_id_version" /><Attribute name = "hgnc_id" /></Dataset></Query>'
#   awk '!$5{print $0,"NA";next}1' ${i}_biomart1.txt > ${i}_biomart1a.txt # fill the 5th column with NA if empty
#   wget -O ${i}_biomart2.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "'${i}'" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "hgnc_symbol" /><Attribute name = "entrezgene_accession" /><Attribute name = "refseq_mrna_predicted" /></Dataset></Query>'
#   awk '!$2{print $0,"NA";next}1' ${i}_biomart2.txt | awk '!$3{print $0,"NA";next}1' | awk '!$4{print $0,"NA";next}1' > ${i}_biomart2a.txt # fill the 2nd, 3rd and 4th column with NA if empty
#   wget -O ${i}_biomart3.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "'${i}'" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "uniprotswissprot" /><Attribute name = "wikigene_name" /></Dataset></Query>'
#   awk '!$2{print $0,"NA";next}1' ${i}_biomart3.txt | awk '!$3{print $0,"NA";next}1' > ${i}_biomart3a.txt # fill the 2nd and 3rd column with NA if empty
#   wget -O ${i}_biomart4.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "'${i}'" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "zfin_id_id" /><Attribute name = "wikigene_id" /></Dataset></Query>'
#   awk '!$2{print $0,"NA";next}1' ${i}_biomart4.txt | awk '!$3{print $0,"NA";next}1' > ${i}_biomart4a.txt # fill the 2nd and 3rd column with NA if empty
#   awk 'BEGIN{OFS="\t"}NR==FNR{a[$3]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA","NA","NA";}}' ${i}_biomart1a.txt ${i}_biomart2a.txt > ${i}_biomart1-2a.txt
#   awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}' ${i}_biomart3a.txt ${i}_biomart1-2a.txt > ${i}_biomart1-2-3a.txt
#   awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}' ${i}_biomart4a.txt ${i}_biomart1-2-3a.txt > ${i}_biomart.tmp.txt
#   printf 'ensembl_transcript_id\thgnc_symbol\tentrezgene_accession\trefseq_mrna_predicted\tensembl_gene_id\tensembl_gene_id_version\tensembl_transcript_id\tensembl_transcript_id_version\thgnc_id\tensembl_transcript_id\tuniprotswissprot\twikigene_name\tensembl_transcript_id\tzfin_id_id\twikigene_id\n' > biomart_headers # NOTE - many of these cols will get removed later
#   cat biomart_headers ${i}_biomart.tmp.txt | awk '{print $1,$2,$3,$4,$5,$6,$8,$9,$11,$12,$14,$15}' OFS='\t' | awk '{print $5,$6,$1,$7,$8,$2,$3,$4,$9,$10,$11,$12}' OFS='\t' > ${i}_biomart.txt
#   rm ${i}_biomart.tmp.txt
# done < $biomartspecies

# # Zebra mbuna genes (M_zebra_UMD2a)
# # wget -O mz_biomart.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "mzebra_gene_ensembl" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "ensembl_gene_id_version" /><Attribute name = "ensembl_transcript_id" /><Attribute name = "ensembl_transcript_id_version" /><Attribute name = "hgnc_id" /><Attribute name = "hgnc_symbol" /><Attribute name = "entrezgene_accession" /><Attribute name = "refseq_mrna_predicted" /><Attribute name = "uniprotswissprot" /><Attribute name = "wikigene_name" /><Attribute name = "zfin_id_id" /><Attribute name = "wikigene_id" /></Dataset></Query>' # this will not work as biomart cannot process this many attributes
#
# wget -O mz_biomart1.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "mzebra_gene_ensembl" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "ensembl_gene_id_version" /><Attribute name = "ensembl_transcript_id" /><Attribute name = "ensembl_transcript_id_version" /><Attribute name = "hgnc_id" /></Dataset></Query>'
# awk '!$5{print $0,"NA";next}1' mz_biomart1.txt > mz_biomart1a.txt # fill the 5th column with NA if empty
#
# wget -O mz_biomart2.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "mzebra_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "hgnc_symbol" /><Attribute name = "entrezgene_accession" /><Attribute name = "refseq_mrna_predicted" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' mz_biomart2.txt | awk '!$3{print $0,"NA";next}1' | awk '!$4{print $0,"NA";next}1' > mz_biomart2a.txt # fill the 2nd, 3rd and 4th column with NA if empty
#
# wget -O mz_biomart3.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "mzebra_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "uniprotswissprot" /><Attribute name = "wikigene_name" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' mz_biomart3.txt | awk '!$3{print $0,"NA";next}1' > mz_biomart3a.txt # fill the 2nd and 3rd column with NA if empty
#
# wget -O mz_biomart4.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "mzebra_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "zfin_id_id" /><Attribute name = "wikigene_id" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' mz_biomart4.txt | awk '!$3{print $0,"NA";next}1' > mz_biomart4a.txt # fill the 2nd and 3rd column with NA if empty
#
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$3]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA","NA","NA";}}' mz_biomart1a.txt mz_biomart2a.txt > mz_biomart1-2a.txt
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}' mz_biomart3a.txt mz_biomart1-2a.txt > mz_biomart1-2-3a.txt
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}' mz_biomart4a.txt mz_biomart1-2-3a.txt > mz_biomart.tmp.txt
#
# printf 'ensembl_transcript_id\thgnc_symbol\tentrezgene_accession\trefseq_mrna_predicted\tensembl_gene_id\tensembl_gene_id_version\tensembl_transcript_id\tensembl_transcript_id_version\thgnc_id\tensembl_transcript_id\tuniprotswissprot\twikigene_name\tensembl_transcript_id\tzfin_id_id\twikigene_id\n' > biomart_headers # NOTE - many of these cols will get removed later
# # printf 'ensembl_gene_id\tensembl_gene_id_version\tensembl_transcript_id\tensembl_transcript_id_version\thgnc_id\tensembl_transcript_id\thgnc_symbol\tentrezgene_accession\trefseq_mrna_predicted\tensembl_transcript_id\tuniprotswissprot\twikigene_name\tensembl_transcript_id\tzfin_id_id\twikigene_id\n' > biomart_headers
# cat biomart_headers mz_biomart.tmp.txt | awk '{print $1,$2,$3,$4,$5,$6,$8,$9,$11,$12,$14,$15}' OFS='\t' | awk '{print $5,$6,$1,$7,$8,$2,$3,$4,$9,$10,$11,$12}' OFS='\t' > mz_biomart.txt
#
#
# # Makobe Island cichlid genes (PunNye1.0)
# # wget -O pn_biomart.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "pnyererei_gene_ensembl" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "ensembl_gene_id_version" /><Attribute name = "ensembl_transcript_id" /><Attribute name = "ensembl_transcript_id_version" /><Attribute name = "hgnc_id" /><Attribute name = "hgnc_symbol" /><Attribute name = "entrezgene_accession" /><Attribute name = "refseq_mrna_predicted" /><Attribute name = "uniprotswissprot" /><Attribute name = "wikigene_name" /><Attribute name = "zfin_id_id" /><Attribute name = "wikigene_id" /></Dataset></Query>' # this will not work as biomart cannot process this many attributes
#
# wget -O pn_biomart1.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "pnyererei_gene_ensembl" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "ensembl_gene_id_version" /><Attribute name = "ensembl_transcript_id" /><Attribute name = "ensembl_transcript_id_version" /><Attribute name = "hgnc_id" /></Dataset></Query>'
# awk '!$5{print $0,"NA";next}1' pn_biomart1.txt > pn_biomart1a.txt # fill the 5th column with NA if empty
#
# wget -O pn_biomart2.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "pnyererei_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "hgnc_symbol" /><Attribute name = "entrezgene_accession" /><Attribute name = "refseq_mrna_predicted" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' pn_biomart2.txt | awk '!$3{print $0,"NA";next}1' | awk '!$4{print $0,"NA";next}1' > pn_biomart2a.txt # fill the 2nd, 3rd and 4th column with NA if empty
#
# wget -O pn_biomart3.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "pnyererei_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "uniprotswissprot" /><Attribute name = "wikigene_name" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' pn_biomart3.txt | awk '!$3{print $0,"NA";next}1' > pn_biomart3a.txt # fill the 2nd and 3rd column with NA if empty
#
# wget -O pn_biomart4.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "pnyererei_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "zfin_id_id" /><Attribute name = "wikigene_id" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' pn_biomart4.txt | awk '!$3{print $0,"NA";next}1' > pn_biomart4a.txt # fill the 2nd and 3rd column with NA if empty
#
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$3]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA","NA","NA";}}' pn_biomart1a.txt pn_biomart2a.txt > pn_biomart1-2a.txt
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}' pn_biomart3a.txt pn_biomart1-2a.txt > pn_biomart1-2-3a.txt
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}' pn_biomart4a.txt pn_biomart1-2-3a.txt > pn_biomart.tmp.txt
#
# printf 'ensembl_transcript_id\thgnc_symbol\tentrezgene_accession\trefseq_mrna_predicted\tensembl_gene_id\tensembl_gene_id_version\tensembl_transcript_id\tensembl_transcript_id_version\thgnc_id\tensembl_transcript_id\tuniprotswissprot\twikigene_name\tensembl_transcript_id\tzfin_id_id\twikigene_id\n' > biomart_headers # NOTE - many of these cols will get removed later
# # printf 'ensembl_gene_id\tensembl_gene_id_version\tensembl_transcript_id\tensembl_transcript_id_version\thgnc_id\tensembl_transcript_id\thgnc_symbol\tentrezgene_accession\trefseq_mrna_predicted\tensembl_transcript_id\tuniprotswissprot\twikigene_name\tensembl_transcript_id\tzfin_id_id\twikigene_id\n' > biomart_headers
# cat biomart_headers pn_biomart.tmp.txt | awk '{print $1,$2,$3,$4,$5,$6,$8,$9,$11,$12,$14,$15}' OFS='\t' | awk '{print $5,$6,$1,$7,$8,$2,$3,$4,$9,$10,$11,$12}' OFS='\t' > pn_biomart.txt
#
#
# # Burton's mouthbrooder genes (AstBur1.0)
# # wget -O ab_biomart.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "hburtoni_gene_ensembl" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "ensembl_gene_id_version" /><Attribute name = "ensembl_transcript_id" /><Attribute name = "ensembl_transcript_id_version" /><Attribute name = "hgnc_id" /><Attribute name = "hgnc_symbol" /><Attribute name = "entrezgene_accession" /><Attribute name = "refseq_mrna_predicted" /><Attribute name = "uniprotswissprot" /><Attribute name = "wikigene_name" /><Attribute name = "zfin_id_id" /><Attribute name = "wikigene_id" /></Dataset></Query>' # this will not work as biomart cannot process this many attributes
#
# wget -O ab_biomart1.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "hburtoni_gene_ensembl" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "ensembl_gene_id_version" /><Attribute name = "ensembl_transcript_id" /><Attribute name = "ensembl_transcript_id_version" /><Attribute name = "hgnc_id" /></Dataset></Query>'
# awk '!$5{print $0,"NA";next}1' ab_biomart1.txt > ab_biomart1a.txt # fill the 5th column with NA if empty
#
# wget -O ab_biomart2.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "hburtoni_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "hgnc_symbol" /><Attribute name = "entrezgene_accession" /><Attribute name = "refseq_mrna_predicted" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' ab_biomart2.txt | awk '!$3{print $0,"NA";next}1' | awk '!$4{print $0,"NA";next}1' > ab_biomart2a.txt # fill the 2nd, 3rd and 4th column with NA if empty
#
# wget -O ab_biomart3.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "hburtoni_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "uniprotswissprot" /><Attribute name = "wikigene_name" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' ab_biomart3.txt | awk '!$3{print $0,"NA";next}1' > ab_biomart3a.txt # fill the 2nd and 3rd column with NA if empty
#
# wget -O ab_biomart4.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "hburtoni_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "zfin_id_id" /><Attribute name = "wikigene_id" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' ab_biomart4.txt | awk '!$3{print $0,"NA";next}1' > ab_biomart4a.txt # fill the 2nd and 3rd column with NA if empty
#
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$3]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA","NA","NA";}}' ab_biomart1a.txt ab_biomart2a.txt > ab_biomart1-2a.txt
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}' ab_biomart3a.txt ab_biomart1-2a.txt > ab_biomart1-2-3a.txt
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}' ab_biomart4a.txt ab_biomart1-2-3a.txt > ab_biomart.tmp.txt
#
# printf 'ensembl_transcript_id\thgnc_symbol\tentrezgene_accession\trefseq_mrna_predicted\tensembl_gene_id\tensembl_gene_id_version\tensembl_transcript_id\tensembl_transcript_id_version\thgnc_id\tensembl_transcript_id\tuniprotswissprot\twikigene_name\tensembl_transcript_id\tzfin_id_id\twikigene_id\n' > biomart_headers # NOTE - many of these cols will get removed later
# # printf 'ensembl_gene_id\tensembl_gene_id_version\tensembl_transcript_id\tensembl_transcript_id_version\thgnc_id\tensembl_transcript_id\thgnc_symbol\tentrezgene_accession\trefseq_mrna_predicted\tensembl_transcript_id\tuniprotswissprot\twikigene_name\tensembl_transcript_id\tzfin_id_id\twikigene_id\n' > biomart_headers
# cat biomart_headers ab_biomart.tmp.txt | awk '{print $1,$2,$3,$4,$5,$6,$8,$9,$11,$12,$14,$15}' OFS='\t' | awk '{print $5,$6,$1,$7,$8,$2,$3,$4,$9,$10,$11,$12}' OFS='\t' > ab_biomart.txt
#
#
# # Lyretail cichlid (NeoBri1.0) - check as this is not in BioMart online (poor annotation!)
# # Since this is not in biomart, just stick with $file1 as the gene alias file
#
# # Nile tilapia genes (O_niloticus_UMD_NMBU)
# # wget -O on_biomart.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "oniloticus_gene_ensembl" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "ensembl_gene_id_version" /><Attribute name = "ensembl_transcript_id" /><Attribute name = "ensembl_transcript_id_version" /><Attribute name = "hgnc_id" /><Attribute name = "hgnc_symbol" /><Attribute name = "entrezgene_accession" /><Attribute name = "refseq_mrna_predicted" /><Attribute name = "uniprotswissprot" /><Attribute name = "wikigene_name" /><Attribute name = "zfin_id_id" /><Attribute name = "wikigene_id" /></Dataset></Query>' # this will not work as biomart cannot process this many attributes
#
# wget -O on_biomart1.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "oniloticus_gene_ensembl" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "ensembl_gene_id_version" /><Attribute name = "ensembl_transcript_id" /><Attribute name = "ensembl_transcript_id_version" /><Attribute name = "hgnc_id" /></Dataset></Query>'
# awk '!$5{print $0,"NA";next}1' on_biomart1.txt > on_biomart1a.txt # fill the 5th column with NA if empty
#
# wget -O on_biomart2.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "oniloticus_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "hgnc_symbol" /><Attribute name = "entrezgene_accession" /><Attribute name = "refseq_mrna_predicted" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' on_biomart2.txt | awk '!$3{print $0,"NA";next}1' | awk '!$4{print $0,"NA";next}1' > on_biomart2a.txt # fill the 2nd, 3rd and 4th column with NA if empty
#
# wget -O on_biomart3.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "oniloticus_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "uniprotswissprot" /><Attribute name = "wikigene_name" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' on_biomart3.txt | awk '!$3{print $0,"NA";next}1' > on_biomart3a.txt # fill the 2nd and 3rd column with NA if empty
#
# wget -O on_biomart4.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "TSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "oniloticus_gene_ensembl" interface = "default" ><Attribute name = "ensembl_transcript_id" /><Attribute name = "zfin_id_id" /><Attribute name = "wikigene_id" /></Dataset></Query>'
# awk '!$2{print $0,"NA";next}1' on_biomart4.txt | awk '!$3{print $0,"NA";next}1' > on_biomart4a.txt # fill the 2nd and 3rd column with NA if empty
#
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$3]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA","NA","NA";}}' on_biomart1a.txt on_biomart2a.txt > on_biomart1-2a.txt
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}' on_biomart3a.txt on_biomart1-2a.txt > on_biomart1-2-3a.txt
# awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA","NA";}}' on_biomart4a.txt on_biomart1-2-3a.txt > on_biomart.tmp.txt
#
# printf 'ensembl_transcript_id\thgnc_symbol\tentrezgene_accession\trefseq_mrna_predicted\tensembl_gene_id\tensembl_gene_id_version\tensembl_transcript_id\tensembl_transcript_id_version\thgnc_id\tensembl_transcript_id\tuniprotswissprot\twikigene_name\tensembl_transcript_id\tzfin_id_id\twikigene_id\n' > biomart_headers # NOTE - many of these cols will get removed later
# # printf 'ensembl_gene_id\tensembl_gene_id_version\tensembl_transcript_id\tensembl_transcript_id_version\thgnc_id\tensembl_transcript_id\thgnc_symbol\tentrezgene_accession\trefseq_mrna_predicted\tensembl_transcript_id\tuniprotswissprot\twikigene_name\tensembl_transcript_id\tzfin_id_id\twikigene_id\n' > biomart_headers
# cat biomart_headers on_biomart.tmp.txt | awk '{print $1,$2,$3,$4,$5,$6,$8,$9,$11,$12,$14,$15}' OFS='\t' | awk '{print $5,$6,$1,$7,$8,$2,$3,$4,$9,$10,$11,$12}' OFS='\t' > on_biomart.txt


# Ac-3. awk match files '$file1' and '$file2' above to create two files:

# Ac-3a. One tab delimited file WITH HEADERS > ${Mz,Pn,Ab,Nb,On}ggenaltsv (these are stored in the species gtf dir), and
sed 's/.gtf.gz/.genealias.txt.tmp1/g' $antfiles | grep -v $removesp | sort -u > $processggenaltsv

while read -r i; do
  echo ${i}_biomart.txt >> $biomartfiles.tmp
done < $biomartspecies
sort -u $biomartfiles.tmp > $biomartfiles; rm $biomartfiles.tmp

while read -u 3 -r file1 && read -u 4 -r file2
do
  awk 'BEGIN{OFS="\t"}NR==FNR{a[$1]=$0;next}{if(a[$1]){print $0,a[$1];}else{print $0,"NA","NA";}}' ${file2} ${file1} | awk '{print $1,$14,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12}' OFS='\t' > $(echo "${file2}" | sed 's/.txt.tmp1/.tsv/g')
done 3<$biomartfiles 4<$processggenaltsv

Nbggenaltsvtmp=$(echo "$annotNbg" | sed 's/.gtf.gz/.genealias.txt.tmp1/g'); cp $Nbggenaltsvtmp $Nbggenaltsv # just copy the tmp1 file and rename for this (since no biomart db exists)

# Ac-3b. Using the file from 'Ac-3a' above, prepare the gene alias files with correct format (plus ensure to exclude any NA/na columns)
# example format (3 col, tab delimited)
# ENSDARG00000031971      kdelc1  ENSDARG00000031971&kdelc1&393905&Dr.82452
# ENSG00000175899 A2M     HGNC:7&A2M&FWP007&S863-7&CPAMD5&BX647329&X68728&M11313&2&MGI:2449119&2408344&9697696&NM_000014&CCDS44827&OTTHUMG00000150267&2&P01023&ENSG00000175899

awk 'NR>1 {print $1,$2,$1"&"$2}' OFS='\t' $Nbggenaltsv | sed 's/\&NA//g' > $Nbggenal

# merge rows with same first column (ensembl_gene_id) and obtain unique cells in each row only for the output
processggenaltsv2=(processggenaltsvpath2.txt)
sed 's/.gtf.gz/.genealias.tsv/g' $antfiles | grep -v $removesp | sort -u > $processggenaltsv2

while read -r i; do
  awk -F'\t' 'NF>1{a[$1] = a[$1]"\t"$0};END{for(i in a)print i"\t"a[i]}' ${i} |
  awk '{ n=split($0,a,FS); $0=""; j=1; delete u; for (i=1; i<=n; i++) if (!u[a[i]]++) $(j++) = a[i]; print }' |
  sed 's/NA//g' > ${i}.tmpfile0
  awk '{print $1,$2}' OFS='\t' ${i}.tmpfile0 > ${i}.tmpfile1
  sed 's/ /\&/g' ${i}.tmpfile0 > ${i}.tmpfile2
  paste ${i}.tmpfile1 ${i}.tmpfile2 | awk '{print $1,$2,$3}' OFS='\t' > $(echo "${i}" | sed 's/.tsv/.txt/g')
  rm ${i}.tmpfile*
done < $processggenaltsv2

sed 's/.tsv/.txt/g' $processggenaltsv2 > $genalpaths
echo $(ls -1 $Nbggenal) >> $genalpaths

while read -r d; do
# for d in "$Mzgannot" "$Pngannot" "$Abgannot" "$Nbgannot" "$Ongannot"; do
  rm $(echo "${d}" | sed 's/.gtf.gz/.genealias.txt.tmp1/g')
done < $antfiles

# B. Create data.config.user file with species specific entries
# see this for using own motifs: https://www.regulatory-genomics.org/motif-analysis/additional-motif-data/
# When RGT is installed, it will automatically create a folder to store additional data (default: ~/rgtdata).
# Within the subfolder motifs, files related to the motif analysis tool will be added: position frequency matrices (describing transcription factor motifs), files needed for the HTML report, etc

echo "$rgtidsp1" >> $rgtdatapath/data.config.user
echo "genome: $FAMzg" >> $rgtdatapath/data.config.user
echo "chromosome_sizes: $Mzgchr" >> $rgtdatapath/data.config.user
echo "gene_regions: $Mzggen" >> $rgtdatapath/data.config.user
echo "annotation: $annotMzg" >> $rgtdatapath/data.config.user
echo "gene_alias: $Mzggenal" >> $rgtdatapath/data.config.user
printf '\n' >> $rgtdatapath/data.config.user

echo "$rgtidsp2" >> $rgtdatapath/data.config.user
echo "genome: $FAPng" >> $rgtdatapath/data.config.user
echo "chromosome_sizes: $Pngchr" >> $rgtdatapath/data.config.user
echo "gene_regions: $Pnggen" >> $rgtdatapath/data.config.user
echo "annotation: $annotPng" >> $rgtdatapath/data.config.user
echo "gene_alias: $Pnggenal" >> $rgtdatapath/data.config.user
printf '\n' >> $rgtdatapath/data.config.user

echo "$rgtidsp3" >> $rgtdatapath/data.config.user
echo "genome: $FAAbg" >> $rgtdatapath/data.config.user
echo "chromosome_sizes: $Abgchr" >> $rgtdatapath/data.config.user
echo "gene_regions: $Abggen" >> $rgtdatapath/data.config.user
echo "annotation: $annotAbg" >> $rgtdatapath/data.config.user
echo "gene_alias: $Abggenal" >> $rgtdatapath/data.config.user
printf '\n' >> $rgtdatapath/data.config.user

echo "$rgtidsp4" >> $rgtdatapath/data.config.user
echo "genome: $FANbg" >> $rgtdatapath/data.config.user
echo "chromosome_sizes: $Nbgchr" >> $rgtdatapath/data.config.user
echo "gene_regions: $Nbggen" >> $rgtdatapath/data.config.user
echo "annotation: $annotNbg" >> $rgtdatapath/data.config.user
echo "gene_alias: $Nbggenal" >> $rgtdatapath/data.config.user
printf '\n' >> $rgtdatapath/data.config.user

echo "$rgtidsp5" >> $rgtdatapath/data.config.user
echo "genome: $FAOng" >> $rgtdatapath/data.config.user
echo "chromosome_sizes: $Ongchr" >> $rgtdatapath/data.config.user
echo "gene_regions: $Onggen" >> $rgtdatapath/data.config.user
echo "annotation: $annotOng" >> $rgtdatapath/data.config.user
echo "gene_alias: $Onggenal" >> $rgtdatapath/data.config.user
printf '\n' >> $rgtdatapath/data.config.user

########## TESTING

# C. Create an array to work on files of each species
scripts=(/tgac/workarea/group-vh/Tarang/ATACseq/2.run2) # already inserted at top
tffprdir=$scripts/2.TFfprint_SignalTrack # already inserted at top
Abpeaks=$tffprdir/abpeakpaths.txt # already inserted at top
cd $tffprdir

# Create narrowPeak file paths for each species - can change species IDs for footprinting here
fpsp1=Mz
fpsp2=Pn
fpsp3=Ab
fpsp4=Nb
fpsp5=On
for Afpsp in "${!fpsp@}"; do
  # echo "$Afpsp is set to ${!Afpsp}"
  ls -1 $idrdir/"${!Afpsp}"*.final.narrowPeak >> $tffprdir/"${!Afpsp}"peakpaths.txt
done

# this will assign peakpaths files to ${fpsp@}peaks variables e.g. $Abpeaks
for Bfpsp in "${!fpsp@}"; do
  # echo "$Bfpsp is set to ${!Bfpsp}"
  eval "${!Bfpsp}"peaks=${tffprdir}/"${!Bfpsp}"peakpaths.txt
done

# get the total number of files for array and assign to variables e.g. $Abpeararrayend for each species, and then assign script variables for running footprinting e.g. $AbFPscript
for Cfpsp in "${!fpsp@}"; do
  # echo "$Cfpsp is set to ${!Cfpsp}"
  eval "${!Cfpsp}"peararrayend=$(wc -l "${!Cfpsp}"peakpaths.txt | awk '{print $1}')
  eval "${!Cfpsp}"FPscript="${!Cfpsp}"_TFfp.sh
done

# D. run TF footprinting and creating signal tracks e.g. https://www.regulatory-genomics.org/hint/tutorial/
# all gtf are zipped (.gz) so check that is ok

# Da. Prepare PWM files/folders and data.config.user to use own motifs and genome info

## Daa. Prepare and move pwm's to specfic path - NOTE: this is relatively hardcoded and thus needs changing for future work

# split to create multiple meme files

pwmsp1=mz
pwmsp2=pn
pwmsp3=ab
pwmsp4=nb
pwmsp5=on
for Apwmsp in "${!pwmsp@}"; do
  # echo "$Apwmsp is set to ${!Apwmsp}"
  mkdir -p $rgtdatapath/motifs/cichlid"${!Apwmsp}"CSsp
  # python3 $splitmeme -i /tgac/workarea/group-vh/Tarang/Cichlid_GRNs/Arboretum_GT_v3/1.TFBSs_v2/FINAL_cichlidPWM_motifs/MouseDerived/2a_CS_"${!Apwmsp}".meme -o $rgtdatapath/motifs/cichlid"${!Apwmsp}"CSsp/tmp -t CS -s "${!Apwmsp}"
  # rm $rgtdatapath/motifs/cichlid"${!Apwmsp}"CSsp/tmp/MEME_CS_"${!Apwmsp}".meme
done

mkdir -p $rgtdatapath/motifs/cichlidCW
# python3 $splitmeme -i /tgac/workarea/group-vh/Tarang/Cichlid_GRNs/Arboretum_GT_v3/1.TFBSs_v2/FINAL_cichlidPWM_motifs/MouseDerived/2b_CW_mz.meme -o $rgtdatapath/motifs/cichlidCW/tmp -t CW -s CW
# rm $rgtdatapath/motifs/cichlidCW/tmp/MEME_CW_CW.meme

mkdir -p $rgtdatapath/motifs/cichlidJASPAR
# python3 $splitmeme -i /tgac/workarea/group-vh/Tarang/Cichlid_GRNs/Arboretum_GT_v3/1.TFBSs_v2/FINAL_cichlidPWM_motifs/MouseDerived/2c_JASPAR_mz.meme -o $rgtdatapath/motifs/cichlidJASPAR/tmp -t CJP -s CJP
# rm $rgtdatapath/motifs/cichlidJASPAR/tmp/MEME_CJP_CJP.meme

# .. Need to convert meme format to JASPAR2016 format - use R universal matrix
for i in /Users/mehtat/Documents/TGAC/Projects/Cichlid_GRNs/Arboretum_GT_v3/1.TFBSs_v2/FINAL_cichlidPWM_motifs/MouseDerived/{CS,CW,JP}/{mz,pn,ab,nb,on}/*.meme; do
  Rscript ~/github/ATAC_bioinformatics/meme2jaspar.R -i ${i} -o "$(echo ${i} | sed 's/.meme/.tmp.pwm/g')"
done

# .. convert JASPAR 2016 CFM PWM to JASPAR OLD PFM format by removing header, nucleotides and square brackets
for i in /Users/mehtat/Documents/TGAC/Projects/Cichlid_GRNs/Arboretum_GT_v3/1.TFBSs_v2/FINAL_cichlidPWM_motifs/MouseDerived/{CS,CW}/{mz,pn,ab,nb,on}/*.tmp.pwm; do
  awk '!/^>/' ${i} | sed 's/A \[  //g' | sed 's/T \[  //g' | sed 's/C \[  //g' | sed 's/G \[  //g' | sed 's/ \]//g' | grep . > "$(echo ${i} | sed 's/.tmp.pwm/.pwm/g')"
  rm ${i}
done
for i in /Users/mehtat/Documents/TGAC/Projects/Cichlid_GRNs/Arboretum_GT_v3/1.TFBSs_v2/FINAL_cichlidPWM_motifs/MouseDerived/JP/mz/*.tmp.pwm; do
  awk '!/^>/' ${i} | sed 's/A \[  //g' | sed 's/T \[  //g' | sed 's/C \[  //g' | sed 's/G \[  //g' | sed 's/ \]//g' | grep . > "$(echo ${i} | sed 's/.tmp.pwm/.pwm/g')"
  rm ${i}
done

# .. move the created PWM to HPC
for i in mz pn ab nb on; do
  cp * $rgtdatapath/motifs/cichlid"${i}"CSsp
done

cp * $rgtdatapath/motifs/cichlidCW
cp * $rgtdatapath/motifs/cichlidJASPAR



# .. Consider making the *.mtf file: see https://www.regulatory-genomics.org/motif-analysis/additional-motif-data/


# mappings here
/Users/mehtat/Documents/TGAC/Projects/Cichlid_GRNs/Arboretum_GT_v3/May2019_ReviewerComments/1.1b.RetinaMotifDivergence/JASPAR_Vertebrates_2018_motifname.OGID.txt


## Dab. generate logos of above
python $rgtdatapath/setupLogoData.py --all # generate logos for all motifs subdirs

echo '[MotifData]' >> $rgtdatapath/data.config.user
echo "pwm_dataset: motifs" >> $rgtdatapath/data.config.user # Contains the path to the motif position weight matrices (PWM) repositories.
echo "logo_dataset: logos" >> $rgtdatapath/data.config.user # Contains the path to the logo graphs (graphical depiction of PWMs). Look here: http://www.regulatory-genomics.org/additional-motif-data/
echo "repositories: cichlidsp, jaspar_vertebrates, hocomoco" >> $rgtdatapath/data.config.user #  	The PWM repositories that will be used in the analyses. It is a comma-separated list of folders inside <pwm_dataset> (see this option above) folder.


# Db. Prepare scripts for footprinting

## Put the echo of script in a for loop below
for Dfpsp in "${!fpsp@}"; do
  # echo "$Dfpsp is set to ${!Dfpsp}"
  "${!Dfpsp}"
done

echo '#!/bin/bash -e' > $AbFPscript
echo '#SBATCH -p ei-medium # partition (queue)' >> $AbFPscript
echo '#SBATCH -N 1 # number of nodes' >> $AbFPscript
echo '#SBATCH -n 1 # number of tasks' >> $AbFPscript
echo '#SBATCH --array=0-'"$Abpeararrayend" >> $AbFPscript
echo '#SBATCH --mem-per-cpu 12000' >> $AbFPscript
echo '#SBATCH -t 0-23:59' >> $AbFPscript
echo '#SBATCH --mail-type=ALL # notifications for job done & fail' >> $AbFPscript
echo '#SBATCH --mail-user=Tarang.Mehta@earlham.ac.uk # send-to address' >> $AbFPscript
echo '#SBATCH -o slurm.%N.%j.out # STDOUT' >> $AbFPscript
echo '#SBATCH -e slurm.%N.%j.err # STDERR' >> $AbFPscript
printf '\n' >> $AbFPscript

scripts=(/tgac/workarea/group-vh/Tarang/ATACseq/2.run2)
ml samtools/1.7
# idrdir=($scripts/1.IDR)
idrdir=($scripts/idr_test) # THIS NEEDS CHANGING TO ABOVE PATH FOR FINAL SCRIPT

# 0. mapfile the narrowPeak files
mapfile -t Abpeakarray < $Abpeaks # assign files to variable for array

# 1. prepare a prefix file using the peaks file e.g. $Abpeaks
Abprefix=(Abprefixes.txt)
awk -F'/' ' { print $NF } ' $Abpeaks | sed 's/_peaks.final.narrowPeak//g' > $Abprefix

# 2. mapfile the above file
mapfile -t Abprefix < $Abprefix

# 3. prepare another file to iterate the input BAM files (can use the array on this) - echo the Abpeakarray and sed replace to prepare the path (will have to use the mapfile from above to add the prefix to peak calling folder)
# $scripts/Ab5_L_ATAC/5.peak_calling/Ab5_L_ATAC.nochrM.nodup.filt.querysorted.bam # this is an example of the input BAM
# /tgac/workarea/group-vh/Tarang/ATACseq/2.run2/idr_test/Ab5_L_ATAC_peaks.final.narrowPeak # this is what the $Abpeaks file is like

AbBAM=(AbinputBAM.txt)
cat ${Abpeakarray[${SLURM_ARRAY_TASK_ID}]} | sed "s|$idrdir|$scripts/${Abprefix[${SLURM_ARRAY_TASK_ID}]}/5.peak_calling|g" | sed 's/_peaks.final.narrowPeak/.nochrM.nodup.filt.querysorted.bam/g' >> $AbBAM
# echo "${Abpeakarray[0]}" | sed "s|$idrdir|$scripts/${Abprefix[0]}/5.peak_calling|g" | sed 's/_peaks.final.narrowPeak/.nochrM.nodup.filt.querysorted.bam/g'

# 4, mapfile the file from above
mapfile -t AbBAM < $AbBAM

# 5. run samtools sort and index and drop in this folder
samtools sort "${AbBAM[${SLURM_ARRAY_TASK_ID}]}" -o "$(basename "${AbBAM[${SLURM_ARRAY_TASK_ID}]}" .querysorted.bam).sorted.bam"
samtools index "$(basename "${AbBAM[${SLURM_ARRAY_TASK_ID}]}" .querysorted.bam).sorted.bam"


# A. call footprints - input BAM is the query indexed and sorted of ATAC reads aligned to genome, mtDNA removed
mkdir -p $tffprdir/${pwmsp3}_fp
rgt-hint footprinting --atac-seq --paired-end --organism=$rgtidsp3 --output-location=$tffprdir/${pwmsp3}_fp/"$(basename "${Abpeakarray[${SLURM_ARRAY_TASK_ID}]}" .final.narrowPeak)FP" --output-prefix=${Abprefix[${SLURM_ARRAY_TASK_ID}]} "$(basename "${AbBAM[${SLURM_ARRAY_TASK_ID}]}" .querysorted.bam).sorted.bam" ${Abpeakarray[${SLURM_ARRAY_TASK_ID}]}

# B. outputs signals for visualization in a genome browser
Absignalprefix=(Absignalprefixes.txt)
sed 's/^/_BC/g' $Abprefix > $Absignalprefix
mapfile -t Absignalprefix < $Absignalprefix
rgt-hint tracks --bc --bigWig --organism=$rgtidsp3 "$(basename "${AbBAM[${SLURM_ARRAY_TASK_ID}]}" .querysorted.bam).sorted.bam" ${Abpeakarray[${SLURM_ARRAY_TASK_ID}]} --output-prefix=${Absignalprefix[${SLURM_ARRAY_TASK_ID}]}

# C. find associated TFs
rgt-motifanalysis matching --filter "database:cichlid${pwmsp3}CSsp, cichlidCW, cichlidJASPAR, jaspar_vertebrates, hocomoco" --organism=$rgtidsp3 --input-files $tffprdir/${pwmsp3}_fp/"$(basename "${Abpeakarray[${SLURM_ARRAY_TASK_ID}]}" .final.narrowPeak)FP"/${Abprefix[${SLURM_ARRAY_TASK_ID}]}.bed







## ~ INSERT CODE HERE ~ ##

echo '# -- 2. FRiP calculation has completed -- #'

echo '# -- 3a. TF footprinting preparation has started - bioMart alias, annotations and data.config prep -- #'


JOBID5=$( sbatch -W --dependency=afterok:${JOBID4} 2.2_biomart_dl.sh | awk '{print $4}' ) # JOB5 depends on JOB4 completing successfully

echo '# -- 3a. TF footprinting preparation has completed - bioMart alias, annotations and data.config prep -- #'

echo '# -- 3b. TF footprinting and creation of signal tracks started -- #'

JOBID6=$( sbatch -W --dependency=afterok:${JOBID5} xx.sh | awk '{print $4}' ) # JOB6 depends on JOB5 completing successfully



################################################################################################################

### 3. Annotation:
# 	3a. TSS enrichment - plot
  # The TSS enrichment calculation is a signal to noise calculation.
  # The reads around a reference set of TSSs are collected to form an aggregate distribution of reads centered on the TSSs and extending to 1000 bp in either direction (for a total of 2000bp).
  # This distribution is then normalized by taking the average read depth in the 100 bps at each of the end flanks of the distribution (for a total of 200bp of averaged data) and calculating a fold change at each position over that average read depth.
  # This means that the flanks should start at 1, and if there is high read signal at transcription start sites (highly open regions of the genome) there should be an increase in signal up to a peak in the middle.
  # We take the signal value at the center of the distribution after this normalization as our TSS enrichment metric.

# 	3b. Fraction of Reads in annotated regions

## ~ INSERT CODE HERE ~ ##

echo '# 3a. TSS enrichment plotting' >> 5.peakcall.sh
echo '# This calls two python scripts - make sure they are in $scripts' >> 5.peakcall.sh
printf '\n' >> 5.peakcall.sh
echo '# create a 2kb window around TSS (+/- 1kb) bed file e.g.' >> 5.peakcall.sh
echo '# chr1	134210701	134214701	+' >> 5.peakcall.sh
echo '# chr1	33724603	33728603	-' >> 5.peakcall.sh
printf '\n' >> 5.peakcall.sh
echo '# 3aA. Protein-coding genes GTF > BED: variable $annot of gzipped GTF file (*gtf.gz); or 2) longest protein-coding gene annotations as 6-column BED: col1-scaff,col2-start,col3-end,col4-geneID,col5-XX,col6-strand' >> 5.peakcall.sh
echo '# output is genebed=($peakcall/$spG'_refGene.bed') defined as variable at top' >> 5.peakcall.sh
printf '\n' >> 5.peakcall.sh
echo 'source bedops-2.4.28' >> 5.peakcall.sh
printf '\n' >> 5.peakcall.sh
echo '# the below is for either one of your own gtf or bed files' >> 5.peakcall.sh
echo "case $annot in" >> 5.peakcall.sh # NOTE: FILES AREN'T ZIPPED ANYMORE!!
echo -e "\t*gtf.gz) gunzip -c $annot | awk 'OFS="'"\\t" {if ($3=="gene") {print $1,$4-1,$5,$10,0,$7,$18}}'"' | tr -d '"'";'"' | grep 'protein_coding' | awk '{print "'$1,$2,$3,$4,$5,$6}'"' OFS="'"\\t" > '"$genebed ;; # GTF > 0-based BED of protein_coding" >> 5.peakcall.sh
echo -e "\t*.bed) awk '{"'print $1,$2,$3,$4,0,$6}'"' OFS="'"\\t"'" $annot > $genebed ;; # BED format ONLY for my files that have six cols (where 6th col is strand)" >> 5.peakcall.sh
echo "esac" >> 5.peakcall.sh
# below is for the old annotations that are a little odd
# echo "case $annot in" >> 5.peakcall.sh
# echo -e "\t*gtf.gz) gunzip -c $annot | awk 'OFS="'"\\t" {if ($3=="gene" || $3=="exon") {print $1,$4-1,$5,$10,0,$7,$18}}'"' | tr -d '"'";'"' | awk '{print "'$1,$2,$3,$4,$5,$6}'"' OFS="'"\\t" > '"$genebed ;; # GTF > 0-based BED of protein_coding" >> 5.peakcall.sh
# echo -e "\t*.bed) awk '{"'print $1,$2,$3,$4,0,$6}'"' OFS="'"\\t"'" $annot > $genebed ;; # BED format ONLY for my files that have six cols (where 6th col is strand)" >> 5.peakcall.sh
# echo "esac" >> 5.peakcall.sh
printf '\n' >> 5.peakcall.sh
# # below is the same as what is echoed above
# case $annot in
#   *gtf.gz) gunzip -c $annot | awk 'OFS="\t" {if ($3=="gene" || $3=="exon") {print $1,$4-1,$5,$10,0,$7,$18}}' | tr -d '";' | awk '{print $1,$2,$3,$4,$5,$6}' OFS='\t' > $genebed ;; # GTF > 0-based BED of protein_coding
#   *.bed) awk '{print $1,$2,$3,$4,0,$6}' OFS='\t' $annot > $genebed ;; # BED format ONLY for my files that have six cols (where 6th col is strand)
# esac
# # the below is for either a gtf file from ensembl etc. and your own bed file
# case $annot in
#   *gtf.gz) gunzip -c $annot | awk 'OFS="\t" {if ($3=="gene" || $3=="exon") {print $1,$4-1,$5,$10,0,$7,$18}}' | tr -d '";' | grep -wiF 'protein_coding' | awk '{print $1,$2,$3,$4,$5,$6}' OFS='\t' > $genebed ;; # GTF > 0-based BED of protein_coding
#   *.bed) awk '{print $1,$2,$3,$4,0,$6}' OFS='\t' $annot > $genebed ;; # BED format ONLY for my files that have six cols (where 6th col is strand)
# esac
echo '# 3aB. Then split them by strand and pad around the stranded-start position of the annotation (taking TSS +/- 1000=1kb)' >> 5.peakcall.sh
echo "awk '("'$6 == "+") { print $0 }'"' $genebed | awk 'BEGIN{ OFS="'"\t" }($2 > 1000){ print $1, ($2 - 1000), ($2 + 1000), $4, $5, $6  }'"' > $genebed.tss.for.padded.bed" >> 5.peakcall.sh
echo "awk '("'$6 == "-") { print $0 }'"' $genebed | awk 'BEGIN{ OFS="'"\t" }($3 > 1000){ print $1, ($3 - 1000), ($3 + 1000), $4, $5, $6  }'"' > $genebed.tss.rev.padded.bed" >> 5.peakcall.sh
echo "bedops --everything $genebed.tss.for.padded.bed $genebed.tss.rev.padded.bed > $genebedtss" >> 5.peakcall.sh
printf '\n' >> 5.peakcall.sh
echo '# 3aC. Keep only TSS regions within chromosomal bounds - prep scaffold sizes file (col1=scaffoldID; col2=0; col3=length) from genome fasta' >> 5.peakcall.sh
echo 'bioawk -c fastx'" '{ print "'$name, length($seq) }'"' < $gFA | awk '{print "'$1,"0",$2}'"' OFS="'"\t" > '"$scafflen" >> 5.peakcall.sh
echo "bedops --element-of 100% $genebedtss $scafflen > $genebedtss2" >> 5.peakcall.sh
printf '\n' >> 5.peakcall.sh
# echo '# 5bD. Use final TSS (+/- 1kb) bed file as input to calculate TSS enrichment and plot with python script ATAC_Bioinf_pipeline_v2b_part5bD.py' >> 5.peakcall.sh
# echo "python3 $scripts/ATAC_Bioinf_pipeline_v2b_part5bD-a.py $fastqr1 # input fastq can be native or gzipped" >> 5.peakcall.sh
# echo "python3 $scripts/ATAC_Bioinf_pipeline_v2b_part5bD.py $Test1 $genebedtss2 $spID $read_len $scafflen"' # usage: python3 $scripts/ATAC_Bioinf_pipeline_v2b_part5bD.py'" 'FINAL_BAM' 'TSS' 'OUTPUT_PREFIX' 'read_len' 'CHROMSIZES'" >> 5.peakcall.sh
# printf '\n' >> 5.peakcall.sh


echo '# -- 3. TF footprinting and creation of signal tracks has completed -- #'

echo '# -- 4. Peak annotation has started -- #'

JOBID7=$( sbatch -W --dependency=afterok:${JOBID6} XX.sh | awk '{print $4}' ) # JOB3 depends on JOB2 completing successfully


################################################################################################################

### 4. Differential analysis of peaks

### NOTE: be careful when considering differential peaks as some may be only offset by a few bases. In this secnario, consider the average number of mapped reads over a window.

## FOR SIMPLE DIFFERENTIAL ANALYSIS OF PEAKS, USE HOMER; SOME CODE HERE: https://dtc-coding-dojo.github.io/main//blog/Analysing_ATAC_and_CHIPseq_data/
## Then use DiffBind to:
  # A. Determine tissue-specific peaks in each species
    # Tissue-specificity of ATAC-seq peaks was determined using DiffBind (https://www.bioconductor.org/packages/release/bioc/ html/DiffBind.html),
    # This provided the peak coordinates for each of the biological replicates of all tissues profiled as input, plus the mapped and shifted sequencing reads (parameters ‘method = DBA_EDGER, bFullLibrarySize = FALSE, bSubControl = FALSE, bTagwise = FALSE’).
    # All peaks identified with a log2 fold change equal or greater than 1 in one tissue compared to all others were selected as tissue-specific.
  # B. Determine tissue-specific peaks between species same tissues e.g. Ab5_L vs Nb5_L
    # you need to find a way to compare different species - use association to orthologous genes?


## ~ INSERT CODE HERE ~ ##

echo '# -- 4. Peak annotation has completed -- #'

echo '# -- 5. Differential analysis of peaks has started -- #'

JOBID8=$( sbatch -W --dependency=afterok:${JOBID7} XX.sh | awk '{print $4}' ) # JOB4 depends on JOB3 completing successfully

################################################################################################################

echo '# -- 5. Differential analysis of peaks has completed -- #'

### Finish the script
exit 0
